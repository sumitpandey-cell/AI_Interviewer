from fastapi import APIRouter, HTTPException, File, UploadFile
from uuid import UUID
from pydantic import BaseModel
from typing import Optional
import base64
from .interview_flow import interview_flow
from ..interviews.schemas import LangGraphState

router = APIRouter()

class ResponseSubmission(BaseModel):
    """Schema for submitting interview responses."""
    response_text: Optional[str] = None
    audio_data: Optional[str] = None  # Base64 encoded audio data
    audio_format: Optional[str] = "webm"  # Audio format (webm, mp3, wav, etc.)

class AudioStreamSubmission(BaseModel):
    """Schema for real-time audio streaming."""
    session_token: str
    audio_chunk: str  # Base64 encoded audio chunk
    chunk_sequence: int  # For ordering chunks
    is_final_chunk: bool = False

@router.post("/start")
async def start_interview(topic: str, user_id: UUID, interview_type: str = "technical", difficulty: str = "medium"):
    """Start an AI interview using the LangGraph workflow system."""
    try:
        # Create initial state
        initial_state = LangGraphState(
            interview_id=0,  # Will be set when saved to DB
            session_token="",  # Will be generated by workflow
            current_step="",
            user_id=int(user_id),
            position=topic,
            interview_type=interview_type,
            difficulty=difficulty
        )
        
        # Start the interview flow
        final_state = await interview_flow.start_interview(initial_state)
        
        return {
            "session_token": final_state.session_token,
            "current_question": final_state.current_question,
            "status": "started",
            "current_step": final_state.current_step,
            "questions_generated": len(final_state.questions_generated) if final_state.questions_generated else 0
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start interview: {str(e)}")

@router.post("/submit-response/{session_token}")
async def submit_response(session_token: str, response: ResponseSubmission):
    """Submit a response to the current interview question."""
    try:
        # In a real implementation, you would load the state from database
        # For now, we'll create a mock state
        current_state = LangGraphState(
            interview_id=1,
            session_token=session_token,
            current_step="waiting_for_response",
            user_id=1,
            position="Software Engineer",
            interview_type="technical",
            difficulty="medium",
            questions_generated=[{"question": "Sample question"}],
            current_question_index=0,
            responses_history=[]
        )
        
        # Handle audio data if present
        user_response_text = response.response_text
        audio_data = None
        
        if response.audio_data:
            # Decode base64 audio data
            try:
                audio_bytes = base64.b64decode(response.audio_data)
                # Store audio bytes temporarily for processing
                audio_data = audio_bytes
                
                # If no text provided, we'll transcribe the audio
                if not user_response_text:
                    user_response_text = "[Audio response - will be transcribed]"
                    
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"Invalid audio data: {str(e)}")
        
        if not user_response_text:
            raise HTTPException(status_code=400, detail="Either response_text or audio_data must be provided")
        
        # Process the response through the flow
        updated_state = await interview_flow.process_response(
            current_state, 
            user_response_text, 
            audio_data=audio_data,
            audio_format=response.audio_format
        )
        
        return {
            "session_token": updated_state.session_token,
            "feedback": getattr(updated_state, 'encouragement_message', 'Response processed'),
            "score": updated_state.ai_evaluation.get("overall_score") if updated_state.ai_evaluation else None,
            "next_question": updated_state.current_question if updated_state.should_continue else None,
            "should_continue": updated_state.should_continue,
            "interview_completed": not updated_state.should_continue,
            "current_step": updated_state.current_step
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to process response: {str(e)}")

@router.post("/submit-audio-file/{session_token}")
async def submit_audio_file(session_token: str, audio_file: UploadFile = File(...)):
    """Submit audio file directly (alternative to base64)."""
    try:
        # Read audio file content
        audio_content = await audio_file.read()
        
        # Get file format from filename or content type
        audio_format = "wav"  # default
        if audio_file.content_type:
            if "webm" in audio_file.content_type:
                audio_format = "webm"
            elif "mp3" in audio_file.content_type:
                audio_format = "mp3"
            elif "ogg" in audio_file.content_type:
                audio_format = "ogg"
        
        # Create mock state
        current_state = LangGraphState(
            interview_id=1,
            session_token=session_token,
            current_step="waiting_for_response",
            user_id=1,
            position="Software Engineer",
            interview_type="technical",
            difficulty="medium",
            questions_generated=[{"question": "Sample question"}],
            current_question_index=0,
            responses_history=[]
        )
        
        # Process the audio response
        updated_state = await interview_flow.process_response(
            current_state, 
            "[Audio response - will be transcribed]",
            audio_data=audio_content,
            audio_format=audio_format
        )
        
        return {
            "session_token": updated_state.session_token,
            "transcription": updated_state.user_response,
            "feedback": getattr(updated_state, 'encouragement_message', 'Response processed'),
            "score": updated_state.ai_evaluation.get("overall_score") if updated_state.ai_evaluation else None,
            "next_question": updated_state.current_question if updated_state.should_continue else None,
            "should_continue": updated_state.should_continue,
            "interview_completed": not updated_state.should_continue,
            "current_step": updated_state.current_step
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to process audio: {str(e)}")

@router.post("/stream-audio")
async def stream_audio_chunk(audio_stream: AudioStreamSubmission):
    """Handle real-time audio streaming chunks."""
    try:
        # Decode audio chunk
        audio_chunk = base64.b64decode(audio_stream.audio_chunk)
        
        # In a real implementation, you would:
        # 1. Store chunks in memory/cache with session_token as key
        # 2. Accumulate chunks until is_final_chunk = True
        # 3. Process complete audio when final chunk received
        
        if audio_stream.is_final_chunk:
            # Process the complete audio
            # For now, return a mock response
            return {
                "session_token": audio_stream.session_token,
                "transcription": "Mock transcription of streamed audio",
                "status": "processed",
                "chunk_count": audio_stream.chunk_sequence + 1
            }
        else:
            # Acknowledge chunk received
            return {
                "session_token": audio_stream.session_token,
                "status": "chunk_received",
                "chunk_sequence": audio_stream.chunk_sequence
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to process audio stream: {str(e)}")

@router.get("/status/{session_token}")
async def get_interview_status(session_token: str):
    """Get the current status of an interview session."""
    try:
        # Mock state for demonstration
        current_state = LangGraphState(
            interview_id=1,
            session_token=session_token,
            current_step="waiting_for_response",
            user_id=1,
            position="Software Engineer",
            interview_type="technical",
            difficulty="medium",
            questions_generated=[{"question": "Sample question"}] * 5,
            responses_history=[],
            should_continue=True
        )
        
        status = await interview_flow.get_interview_status(current_state)
        return status
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get interview status: {str(e)}")
